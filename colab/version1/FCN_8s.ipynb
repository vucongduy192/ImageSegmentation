{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "FCN-8s.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1fvF_HDFabQ",
        "colab_type": "text"
      },
      "source": [
        "## **Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifDCIePDP-sV",
        "colab_type": "code",
        "outputId": "fdab17eb-0e64-485c-87bf-86d2a040fbf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKC1GVZFlhDL",
        "colab_type": "code",
        "outputId": "aa11c1b3-85cc-430a-e313-619fa8fd051d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "from tensorflow.python.keras import backend as K\n",
        "assert K.image_data_format() == 'channels_last'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcP0A-fhw_0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow as tf\n",
        "# %load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyFPCVXiwEEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %tensorboard --logdir '/content/drive/My Drive/dataset/FCN/graphs'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu5nzExFW2jg",
        "colab_type": "code",
        "outputId": "fbee4ebb-d948-4809-cbcf-197326625447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from glob import glob\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "print('Tensorflow version: {}'.format(tf.__version__))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deQykVjATHuQ",
        "colab_type": "text"
      },
      "source": [
        "## **FCN_VGG**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg79-AC2TBRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FCNVGG:\n",
        "    # ---------------------------------------------------------------------------\n",
        "    def __init__(self, session, num_classes):\n",
        "        self.session = session\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build_from_metagraph(self, metagraph_file, checkpoint_file):\n",
        "        self.__load_fcnvgg(metagraph_file, checkpoint_file)\n",
        "\n",
        "    def build_from_vgg(self, vgg_path):\n",
        "        self.__load_vgg(vgg_path)\n",
        "        self.__make_result_tensors()\n",
        "\n",
        "    def __load_fcnvgg(self, metagraph_file, checkpoint_file):\n",
        "        saver = tf.train.import_meta_graph(metagraph_file)\n",
        "        saver.restore(self.session, checkpoint_file)\n",
        "\n",
        "        self.image_input = self.session.graph.get_tensor_by_name('image_input:0')\n",
        "        self.keep_prob = self.session.graph.get_tensor_by_name('keep_prob:0')\n",
        "        self.logits      = self.session.graph.get_tensor_by_name('sum/fcn_logits:0')\n",
        "        self.softmax     = self.session.graph.get_tensor_by_name('result/fcn_softmax:0')\n",
        "        self.classes     = self.session.graph.get_tensor_by_name('result/fcn_class:0')\n",
        "\n",
        "\n",
        "    def __load_vgg(self, vgg_path):\n",
        "        model = tf.saved_model.loader.load(self.session, ['vgg16'], vgg_path)\n",
        "        graph = tf.get_default_graph()\n",
        "\n",
        "        self.image_input = graph.get_tensor_by_name('image_input:0')\n",
        "        self.keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
        "        self.vgg_layer3 = graph.get_tensor_by_name('layer3_out:0')\n",
        "        self.vgg_layer4 = graph.get_tensor_by_name('layer4_out:0')\n",
        "        self.vgg_layer7 = graph.get_tensor_by_name('layer7_out:0')\n",
        "\n",
        "    def __make_result_tensors(self):\n",
        "        \"\"\"\n",
        "          :param correct_label:\n",
        "          :param num_classes:\n",
        "          :return:\n",
        "        \"\"\"\n",
        "        # Use a shorter variable name for simplicity\n",
        "        layer3, layer4, layer7 = self.vgg_layer3, self.vgg_layer4, self.vgg_layer7\n",
        "        # Convert FCN to CONV\n",
        "        # Apply 1x1 convolution in place of fully connected layer\n",
        "        fcn8 = tf.layers.conv2d(layer7, filters=self.num_classes+1, kernel_size=1, name=\"fcn8\")\n",
        "        # Up-sample fcn8 with size depth=(4096?) to match size of layer 4\n",
        "        # so that we can add skip connection with 4th layer\n",
        "        fcn9 = tf.layers.conv2d_transpose(fcn8, filters=layer4.get_shape().as_list()[-1],\n",
        "                                          kernel_size=4, strides=(2, 2), padding='SAME', name=\"fcn9\")\n",
        "        # Add a skip connection between current final layer fcn8 and 4th layer\n",
        "        fcn9_skip_connected = tf.add(fcn9, layer4, name=\"fcn9_plus_vgg_layer4\")\n",
        "        # print(fcn9_skip_connected.shape)\n",
        "        # Up-sample again\n",
        "        fcn10 = tf.layers.conv2d_transpose(fcn9_skip_connected, filters=layer3.get_shape().as_list()[-1],\n",
        "                                           kernel_size=4, strides=(2, 2), padding='SAME', name=\"fcn10_conv2d\")\n",
        "\n",
        "        # Add skip connection\n",
        "        fcn10_skip_connected = tf.add(fcn10, layer3, name=\"fcn10_plus_vgg_layer3\")\n",
        "\n",
        "        # Up-sample again\n",
        "        # Final output: fcn11 = 8 * (4 * layer_out7 + 2 * layer_out4 + layer_out3)\n",
        "        fcn11 = tf.layers.conv2d_transpose(fcn10_skip_connected, filters=self.num_classes+1,\n",
        "                                           kernel_size=16, strides=(8, 8), padding='SAME', name=\"fcn11\")\n",
        "\n",
        "        with tf.variable_scope('sum'):\n",
        "            self.logits = tf.reshape(fcn11, (-1, self.num_classes), name=\"fcn_logits\")\n",
        "\n",
        "        with tf.name_scope('result'):\n",
        "            self.softmax = tf.nn.softmax(self.logits, name=\"fcn_softmax\")\n",
        "            self.classes = tf.argmax(self.softmax, axis=-1, name=\"fcn_class\")\n",
        "\n",
        "\n",
        "    def get_optimizer(self, labels, learning_rate=0.0001):\n",
        "        with tf.variable_scope('reshape'):\n",
        "            labels_reshaped  = tf.reshape(labels, [-1, self.num_classes])\n",
        "            logits_reshaped  = tf.reshape(self.logits, [-1, self.num_classes])\n",
        "\n",
        "            losses          = tf.nn.softmax_cross_entropy_with_logits(\n",
        "                                  labels=labels_reshaped,\n",
        "                                  logits=logits_reshaped)\n",
        "            loss            = tf.reduce_mean(losses)\n",
        "\n",
        "        with tf.variable_scope('optimizer'):\n",
        "            optimizer       = tf.train.AdamOptimizer(learning_rate)\n",
        "            optimizer       = optimizer.minimize(loss)\n",
        "\n",
        "        return optimizer, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-YRYWGATTDH",
        "colab_type": "text"
      },
      "source": [
        "## **Source_VOC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdOoUmU4TWyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import cv2\n",
        "import os\n",
        "# from skimage.io import imshow\n",
        "# import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def color_map(N=256, normalized=False):\n",
        "    def bitget(byteval, idx):\n",
        "        return (byteval & (1 << idx)) != 0\n",
        "\n",
        "    dtype = 'float32' if normalized else 'uint8'\n",
        "    cmap = np.zeros((N, 3), dtype=dtype)\n",
        "    for i in range(N):\n",
        "        r = g = b = 0\n",
        "        c = i\n",
        "        for j in range(8):\n",
        "            r = r | (bitget(c, 0) << 7 - j)\n",
        "            g = g | (bitget(c, 1) << 7 - j)\n",
        "            b = b | (bitget(c, 2) << 7 - j)\n",
        "            c = c >> 3\n",
        "\n",
        "        cmap[i] = np.array([b, g, r])\n",
        "\n",
        "    cmap = cmap / 255 if normalized else cmap\n",
        "    return cmap\n",
        "\n",
        "\n",
        "def color_map_dict():\n",
        "    labels = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair',\n",
        "              'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n",
        "              'train', 'tvmonitor', 'void']\n",
        "    \n",
        "    return dict(zip(labels, color_map()))\n",
        "\n",
        "\n",
        "class VOCSource(object):\n",
        "    def __init__(self):\n",
        "        self.num_training = None\n",
        "        self.num_validation = None\n",
        "        self.train_generator = None\n",
        "        self.valid_generator = None\n",
        "\n",
        "        self.num_testing = None\n",
        "        self.test_generator = None\n",
        "\n",
        "        self.label_colors = color_map_dict()\n",
        "        self.num_classes = 21   # ko bao gá»“m class void\n",
        "        self.image_size = (512, 512)\n",
        "\n",
        "    # -------------------------------------------------------------------------------\n",
        "    def load_data(self, data_dir, images_txt, validation_size=None):\n",
        "        images_dir = os.path.join(data_dir, 'VOC2012/JPEGImages/')   # very large\n",
        "        labels_dir = os.path.join(data_dir, 'VOC2012/SegmentationClass/')  # 2913\n",
        "\n",
        "        # train.txt: 1464 , val.txt: 1449 , Segmentation: 2913\n",
        "        with open(images_txt, 'r') as f:\n",
        "            image_names = f.readlines()\n",
        "            image_paths, label_paths = [], {}\n",
        "            for image in image_names:\n",
        "                image_path = images_dir + image[:-1] + '.jpg'\n",
        "                image_paths.append(image_path)\n",
        "                label_paths[os.path.basename(image_path)] = labels_dir + image[:-1] + '.png'\n",
        "        \n",
        "        random.shuffle(image_paths)\n",
        "        if validation_size is None:\n",
        "            test_images = image_paths\n",
        "            self.num_testing = len(test_images)\n",
        "            self.test_generator = self.batch_generator(test_images, label_paths)\n",
        "        else:\n",
        "            num_images = len(image_paths)\n",
        "            valid_images = image_paths[:int(validation_size * num_images)]\n",
        "            train_images = image_paths[int(validation_size * num_images):]\n",
        "\n",
        "            self.num_training = len(train_images)\n",
        "            self.num_validation = len(valid_images)\n",
        "            self.train_generator = self.batch_generator(train_images, label_paths)\n",
        "            self.valid_generator = self.batch_generator(valid_images, label_paths)\n",
        "\n",
        "    # -------------------------------------------------------------------------------\n",
        "    def batch_generator(self, image_paths, label_paths):\n",
        "        def gen_batch(batch_size):\n",
        "            \"\"\"\n",
        "            :param batch_size:\n",
        "            :return: generator of a batch contain (images, labels)\n",
        "            \"\"\"\n",
        "            random.shuffle(image_paths)\n",
        "            for offset in range(0, len(image_paths), batch_size):\n",
        "                files = image_paths[offset:(offset + batch_size)]\n",
        "                images = []\n",
        "                labels = []\n",
        "                for image_file in files:\n",
        "                    label_file = label_paths[os.path.basename(image_file)]  # base name get file name from path\n",
        "                    image = cv2.resize(cv2.imread(image_file), self.image_size)\n",
        "                    label = cv2.resize(cv2.imread(label_file), self.image_size)\n",
        "\n",
        "                    label_bg   = np.zeros([image.shape[0], image.shape[1]], dtype=bool)\n",
        "                    label_list = []\n",
        "                    for obj, color_bgr in self.label_colors.items():\n",
        "                        if obj == 'background':\n",
        "                            continue\n",
        "                        label_current  = np.all(label == color_bgr, axis=2)\n",
        "                        label_bg      |= label_current\n",
        "                        label_list.append(label_current)\n",
        "\n",
        "                    label_bg   = ~label_bg\n",
        "                    label_all  = np.dstack([label_bg, *label_list])\n",
        "                    label_all  = label_all.astype(np.float32)\n",
        "\n",
        "                    images.append(image.astype(np.float32))\n",
        "                    labels.append(label_all)\n",
        "\n",
        "                yield np.array(images), np.array(labels)\n",
        "        return gen_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbvcToWJTgeO",
        "colab_type": "text"
      },
      "source": [
        "## **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwUKO8OWTagu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from source_kitti import KittiSource\n",
        "# from source_voc import VOCSource\n",
        "# from fcnvgg import FCNVGG\n",
        "\n",
        "\n",
        "def load_kitti_source():\n",
        "    return KittiSource()\n",
        "\n",
        "\n",
        "def load_voc_source():\n",
        "    return VOCSource()\n",
        "\n",
        "\n",
        "def load_fcnvgg(session, num_classes):\n",
        "    return FCNVGG(session, num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoEQRagqTvFX",
        "colab_type": "text"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqvwyG0lTkF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Killing optional CPU driver warnings\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "# Specify all directory paths\n",
        "data_dir = '/content/drive/My Drive/dataset/FCN/VOCdevkit/'\n",
        "vgg_dir = '/content/drive/My Drive/dataset/FCN/vgg/'\n",
        "log_dir = '/content/drive/My Drive/dataset/FCN/graphs/'\n",
        "model_dir = '/content/drive/My Drive/dataset/FCN/saved_model/'\n",
        "\n",
        "images_txt = '/content/drive/My Drive/dataset/FCN/VOCdevkit/VOC2012/ImageSets/Segmentation/train.txt'\n",
        "\n",
        "saved_model_dir = '/content/drive/My Drive/dataset/FCN/saved_model/20200212-095201/'\n",
        "checkpoint_file = os.path.join(saved_model_dir, 'checkpoint')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv4rlVjipBSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    source = load_voc_source()\n",
        "    source.load_data(data_dir, images_txt, validation_size=0.2)\n",
        "    train_generator = source.train_generator\n",
        "    valid_generator = source.valid_generator\n",
        "\n",
        "    epochs = 120\n",
        "    batch_size = 12\n",
        "\n",
        "    # Declare some placeholder will use when training\n",
        "    correct_label = tf.placeholder(tf.float32, [None, source.image_size[1], \n",
        "                                                source.image_size[0], source.num_classes])\n",
        "    training_loss = tf.placeholder(tf.float32)\n",
        "    training_loss_summary_op = tf.summary.scalar('training_loss',\n",
        "                                            training_loss)\n",
        "    validation_loss = tf.placeholder(tf.float32)\n",
        "    validation_loss_summary_op = tf.summary.scalar('validation_loss',\n",
        "                                              validation_loss)\n",
        "\n",
        "    session_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "    with tf.Session(config=session_config) as session:\n",
        "        # Create instance of FCNVGG model\n",
        "        net = load_fcnvgg(session, source.num_classes)\n",
        "        checkpoint_dir = os.path.join(model_dir, datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "        \n",
        "        net.build_from_vgg(vgg_dir)\n",
        "        saver = tf.train.Saver(max_to_keep=10)\n",
        "        print(\"Model build successful, starting train\")\n",
        "\n",
        "        optimizer_op, loss_op = net.get_optimizer(correct_label, learning_rate=0.0001)\n",
        "        loss_summary = tf.summary.scalar(\"Loss\", loss_op)\n",
        "\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        for epoch in range(1, epochs+1, 1):\n",
        "            print(\"STARTING EPOCH {} ...\".format(epoch))\n",
        "            # ---------------------------------- #\n",
        "            # Training process\n",
        "            training_loss_total = 0\n",
        "            generator = train_generator(batch_size)\n",
        "            for X_batch, gt_batch in generator:\n",
        "                _, loss = session.run([optimizer_op, loss_op],\n",
        "                                      feed_dict={net.image_input: X_batch, correct_label: gt_batch,\n",
        "                                                 net.keep_prob: 0.5})\n",
        "\n",
        "                training_loss_total += loss * X_batch.shape[0]\n",
        "            training_loss_total /= source.num_training\n",
        "\n",
        "            # ----------------------------------- #\n",
        "            # Validation process\n",
        "            valid_loss_total = 0\n",
        "            generator = valid_generator(batch_size)\n",
        "            for X_batch, gt_batch in generator:\n",
        "                _, loss = session.run([optimizer_op, loss_op],\n",
        "                                      feed_dict={net.image_input: X_batch, correct_label: gt_batch,\n",
        "                                                 net.keep_prob: 1.})\n",
        "\n",
        "                valid_loss_total += loss * X_batch.shape[0]\n",
        "            valid_loss_total /= source.num_validation\n",
        "            # ----------------------------------- #\n",
        "\n",
        "            print(\"EPOCH {} ...\".format(epoch))\n",
        "            print(\"Training loss = {:.3f}\".format(training_loss_total))\n",
        "            print(\"Validation loss = {:.3f}\".format(valid_loss_total))\n",
        "\n",
        "            # ----------------------------------- #\n",
        "            # Write loss summary\n",
        "            feed = {validation_loss: valid_loss_total,\n",
        "                    training_loss: training_loss_total}\n",
        "            loss_summary = session.run([training_loss_summary_op,\n",
        "                                        validation_loss_summary_op],\n",
        "                                       feed_dict=feed)\n",
        "            summary_op = tf.summary.merge([loss_summary[0], loss_summary[1]])\n",
        "            # writer.add_summary(summary_op.eval(), epoch)\n",
        "\n",
        "            if epoch % 40 == 0:\n",
        "                checkpoint = checkpoint_dir + '/epoch{}.ckpt'.format(epoch)\n",
        "                saver.save(session, checkpoint)\n",
        "                print('Checkpoint saved:', checkpoint)\n",
        "            # ----------------------------------- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzsFFBQ0jcMK",
        "colab_type": "text"
      },
      "source": [
        "# **Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8dbHmWxgV2W",
        "colab_type": "code",
        "outputId": "384e7351-639e-45f2-86c9-a6984bef9298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-c4ac776f0067>:26: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/dataset/FCN/vgg/variables/variables\n",
            "WARNING:tensorflow:From <ipython-input-6-c4ac776f0067>:45: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-c4ac776f0067>:49: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "Model build successful, starting train\n",
            "WARNING:tensorflow:From <ipython-input-6-c4ac776f0067>:80: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "STARTING EPOCH 1 ...\n",
            "EPOCH 1 ...\n",
            "Training loss = 9.749\n",
            "Validation loss = 1.265\n",
            "STARTING EPOCH 2 ...\n",
            "EPOCH 2 ...\n",
            "Training loss = 1.231\n",
            "Validation loss = 1.185\n",
            "STARTING EPOCH 3 ...\n",
            "EPOCH 3 ...\n",
            "Training loss = 1.175\n",
            "Validation loss = 1.144\n",
            "STARTING EPOCH 4 ...\n",
            "EPOCH 4 ...\n",
            "Training loss = 1.154\n",
            "Validation loss = 1.114\n",
            "STARTING EPOCH 5 ...\n",
            "EPOCH 5 ...\n",
            "Training loss = 1.175\n",
            "Validation loss = 1.121\n",
            "STARTING EPOCH 6 ...\n",
            "EPOCH 6 ...\n",
            "Training loss = 1.149\n",
            "Validation loss = 1.121\n",
            "STARTING EPOCH 7 ...\n",
            "EPOCH 7 ...\n",
            "Training loss = 1.125\n",
            "Validation loss = 1.088\n",
            "STARTING EPOCH 8 ...\n",
            "EPOCH 8 ...\n",
            "Training loss = 1.104\n",
            "Validation loss = 1.073\n",
            "STARTING EPOCH 9 ...\n",
            "EPOCH 9 ...\n",
            "Training loss = 1.089\n",
            "Validation loss = 1.077\n",
            "STARTING EPOCH 10 ...\n",
            "EPOCH 10 ...\n",
            "Training loss = 1.065\n",
            "Validation loss = 1.048\n",
            "STARTING EPOCH 11 ...\n",
            "EPOCH 11 ...\n",
            "Training loss = 1.053\n",
            "Validation loss = 0.999\n",
            "STARTING EPOCH 12 ...\n",
            "EPOCH 12 ...\n",
            "Training loss = 1.017\n",
            "Validation loss = 1.032\n",
            "STARTING EPOCH 13 ...\n",
            "EPOCH 13 ...\n",
            "Training loss = 0.984\n",
            "Validation loss = 0.948\n",
            "STARTING EPOCH 14 ...\n",
            "EPOCH 14 ...\n",
            "Training loss = 0.972\n",
            "Validation loss = 0.957\n",
            "STARTING EPOCH 15 ...\n",
            "EPOCH 15 ...\n",
            "Training loss = 0.946\n",
            "Validation loss = 0.876\n",
            "STARTING EPOCH 16 ...\n",
            "EPOCH 16 ...\n",
            "Training loss = 0.912\n",
            "Validation loss = 0.875\n",
            "STARTING EPOCH 17 ...\n",
            "EPOCH 17 ...\n",
            "Training loss = 0.896\n",
            "Validation loss = 0.865\n",
            "STARTING EPOCH 18 ...\n",
            "EPOCH 18 ...\n",
            "Training loss = 0.877\n",
            "Validation loss = 0.796\n",
            "STARTING EPOCH 19 ...\n",
            "EPOCH 19 ...\n",
            "Training loss = 0.812\n",
            "Validation loss = 0.772\n",
            "STARTING EPOCH 20 ...\n",
            "EPOCH 20 ...\n",
            "Training loss = 0.803\n",
            "Validation loss = 0.771\n",
            "STARTING EPOCH 21 ...\n",
            "EPOCH 21 ...\n",
            "Training loss = 0.748\n",
            "Validation loss = 0.714\n",
            "STARTING EPOCH 22 ...\n",
            "EPOCH 22 ...\n",
            "Training loss = 0.727\n",
            "Validation loss = 0.667\n",
            "STARTING EPOCH 23 ...\n",
            "EPOCH 23 ...\n",
            "Training loss = 0.689\n",
            "Validation loss = 0.689\n",
            "STARTING EPOCH 24 ...\n",
            "EPOCH 24 ...\n",
            "Training loss = 0.638\n",
            "Validation loss = 0.590\n",
            "STARTING EPOCH 25 ...\n",
            "EPOCH 25 ...\n",
            "Training loss = 0.597\n",
            "Validation loss = 0.580\n",
            "STARTING EPOCH 26 ...\n",
            "EPOCH 26 ...\n",
            "Training loss = 0.566\n",
            "Validation loss = 0.549\n",
            "STARTING EPOCH 27 ...\n",
            "EPOCH 27 ...\n",
            "Training loss = 0.529\n",
            "Validation loss = 0.491\n",
            "STARTING EPOCH 28 ...\n",
            "EPOCH 28 ...\n",
            "Training loss = 0.477\n",
            "Validation loss = 0.489\n",
            "STARTING EPOCH 29 ...\n",
            "EPOCH 29 ...\n",
            "Training loss = 0.461\n",
            "Validation loss = 0.442\n",
            "STARTING EPOCH 30 ...\n",
            "EPOCH 30 ...\n",
            "Training loss = 0.382\n",
            "Validation loss = 0.347\n",
            "STARTING EPOCH 31 ...\n",
            "EPOCH 31 ...\n",
            "Training loss = 0.362\n",
            "Validation loss = 0.304\n",
            "STARTING EPOCH 32 ...\n",
            "EPOCH 32 ...\n",
            "Training loss = 0.316\n",
            "Validation loss = 0.303\n",
            "STARTING EPOCH 33 ...\n",
            "EPOCH 33 ...\n",
            "Training loss = 0.367\n",
            "Validation loss = 0.270\n",
            "STARTING EPOCH 34 ...\n",
            "EPOCH 34 ...\n",
            "Training loss = 0.263\n",
            "Validation loss = 0.218\n",
            "STARTING EPOCH 35 ...\n",
            "EPOCH 35 ...\n",
            "Training loss = 0.232\n",
            "Validation loss = 0.192\n",
            "STARTING EPOCH 36 ...\n",
            "EPOCH 36 ...\n",
            "Training loss = 0.212\n",
            "Validation loss = 0.185\n",
            "STARTING EPOCH 37 ...\n",
            "EPOCH 37 ...\n",
            "Training loss = 0.191\n",
            "Validation loss = 0.176\n",
            "STARTING EPOCH 38 ...\n",
            "EPOCH 38 ...\n",
            "Training loss = 0.175\n",
            "Validation loss = 0.161\n",
            "STARTING EPOCH 39 ...\n",
            "EPOCH 39 ...\n",
            "Training loss = 0.167\n",
            "Validation loss = 0.150\n",
            "STARTING EPOCH 40 ...\n",
            "EPOCH 40 ...\n",
            "Training loss = 0.161\n",
            "Validation loss = 0.149\n",
            "Checkpoint saved: /content/drive/My Drive/dataset/FCN/saved_model/20200213-075842/epoch40.ckpt\n",
            "STARTING EPOCH 41 ...\n",
            "EPOCH 41 ...\n",
            "Training loss = 0.160\n",
            "Validation loss = 0.141\n",
            "STARTING EPOCH 42 ...\n",
            "EPOCH 42 ...\n",
            "Training loss = 0.153\n",
            "Validation loss = 0.132\n",
            "STARTING EPOCH 43 ...\n",
            "EPOCH 43 ...\n",
            "Training loss = 0.147\n",
            "Validation loss = 0.135\n",
            "STARTING EPOCH 44 ...\n",
            "EPOCH 44 ...\n",
            "Training loss = 0.141\n",
            "Validation loss = 0.130\n",
            "STARTING EPOCH 45 ...\n",
            "EPOCH 45 ...\n",
            "Training loss = 0.140\n",
            "Validation loss = 0.130\n",
            "STARTING EPOCH 46 ...\n",
            "EPOCH 46 ...\n",
            "Training loss = 0.136\n",
            "Validation loss = 0.125\n",
            "STARTING EPOCH 47 ...\n",
            "EPOCH 47 ...\n",
            "Training loss = 0.135\n",
            "Validation loss = 0.120\n",
            "STARTING EPOCH 48 ...\n",
            "EPOCH 48 ...\n",
            "Training loss = 0.129\n",
            "Validation loss = 0.118\n",
            "STARTING EPOCH 49 ...\n",
            "EPOCH 49 ...\n",
            "Training loss = 0.125\n",
            "Validation loss = 0.114\n",
            "STARTING EPOCH 50 ...\n",
            "EPOCH 50 ...\n",
            "Training loss = 0.123\n",
            "Validation loss = 0.112\n",
            "STARTING EPOCH 51 ...\n",
            "EPOCH 51 ...\n",
            "Training loss = 0.123\n",
            "Validation loss = 0.113\n",
            "STARTING EPOCH 52 ...\n",
            "EPOCH 52 ...\n",
            "Training loss = 0.119\n",
            "Validation loss = 0.114\n",
            "STARTING EPOCH 53 ...\n",
            "EPOCH 53 ...\n",
            "Training loss = 0.120\n",
            "Validation loss = 0.110\n",
            "STARTING EPOCH 54 ...\n",
            "EPOCH 54 ...\n",
            "Training loss = 0.118\n",
            "Validation loss = 0.112\n",
            "STARTING EPOCH 55 ...\n",
            "EPOCH 55 ...\n",
            "Training loss = 0.115\n",
            "Validation loss = 0.106\n",
            "STARTING EPOCH 56 ...\n",
            "EPOCH 56 ...\n",
            "Training loss = 0.109\n",
            "Validation loss = 0.105\n",
            "STARTING EPOCH 57 ...\n",
            "EPOCH 57 ...\n",
            "Training loss = 0.111\n",
            "Validation loss = 0.108\n",
            "STARTING EPOCH 58 ...\n",
            "EPOCH 58 ...\n",
            "Training loss = 0.113\n",
            "Validation loss = 0.105\n",
            "STARTING EPOCH 59 ...\n",
            "EPOCH 59 ...\n",
            "Training loss = 0.377\n",
            "Validation loss = 0.677\n",
            "STARTING EPOCH 60 ...\n",
            "EPOCH 60 ...\n",
            "Training loss = 0.408\n",
            "Validation loss = 0.229\n",
            "STARTING EPOCH 61 ...\n",
            "EPOCH 61 ...\n",
            "Training loss = 0.198\n",
            "Validation loss = 0.133\n",
            "STARTING EPOCH 62 ...\n",
            "EPOCH 62 ...\n",
            "Training loss = 0.131\n",
            "Validation loss = 0.103\n",
            "STARTING EPOCH 63 ...\n",
            "EPOCH 63 ...\n",
            "Training loss = 0.108\n",
            "Validation loss = 0.092\n",
            "STARTING EPOCH 64 ...\n",
            "EPOCH 64 ...\n",
            "Training loss = 0.100\n",
            "Validation loss = 0.086\n",
            "STARTING EPOCH 65 ...\n",
            "EPOCH 65 ...\n",
            "Training loss = 0.100\n",
            "Validation loss = 0.085\n",
            "STARTING EPOCH 66 ...\n",
            "EPOCH 66 ...\n",
            "Training loss = 0.097\n",
            "Validation loss = 0.088\n",
            "STARTING EPOCH 67 ...\n",
            "EPOCH 67 ...\n",
            "Training loss = 0.096\n",
            "Validation loss = 0.086\n",
            "STARTING EPOCH 68 ...\n",
            "EPOCH 68 ...\n",
            "Training loss = 0.095\n",
            "Validation loss = 0.086\n",
            "STARTING EPOCH 69 ...\n",
            "EPOCH 69 ...\n",
            "Training loss = 0.092\n",
            "Validation loss = 0.081\n",
            "STARTING EPOCH 70 ...\n",
            "EPOCH 70 ...\n",
            "Training loss = 0.091\n",
            "Validation loss = 0.076\n",
            "STARTING EPOCH 71 ...\n",
            "EPOCH 71 ...\n",
            "Training loss = 0.089\n",
            "Validation loss = 0.076\n",
            "STARTING EPOCH 72 ...\n",
            "EPOCH 72 ...\n",
            "Training loss = 0.088\n",
            "Validation loss = 0.077\n",
            "STARTING EPOCH 73 ...\n",
            "EPOCH 73 ...\n",
            "Training loss = 0.087\n",
            "Validation loss = 0.078\n",
            "STARTING EPOCH 74 ...\n",
            "EPOCH 74 ...\n",
            "Training loss = 0.087\n",
            "Validation loss = 0.078\n",
            "STARTING EPOCH 75 ...\n",
            "EPOCH 75 ...\n",
            "Training loss = 0.085\n",
            "Validation loss = 0.076\n",
            "STARTING EPOCH 76 ...\n",
            "EPOCH 76 ...\n",
            "Training loss = 0.085\n",
            "Validation loss = 0.077\n",
            "STARTING EPOCH 77 ...\n",
            "EPOCH 77 ...\n",
            "Training loss = 0.086\n",
            "Validation loss = 0.080\n",
            "STARTING EPOCH 78 ...\n",
            "EPOCH 78 ...\n",
            "Training loss = 0.085\n",
            "Validation loss = 0.082\n",
            "STARTING EPOCH 79 ...\n",
            "EPOCH 79 ...\n",
            "Training loss = 0.084\n",
            "Validation loss = 0.083\n",
            "STARTING EPOCH 80 ...\n",
            "EPOCH 80 ...\n",
            "Training loss = 0.086\n",
            "Validation loss = 0.086\n",
            "Checkpoint saved: /content/drive/My Drive/dataset/FCN/saved_model/20200213-075842/epoch80.ckpt\n",
            "STARTING EPOCH 81 ...\n",
            "EPOCH 81 ...\n",
            "Training loss = 0.082\n",
            "Validation loss = 0.078\n",
            "STARTING EPOCH 82 ...\n",
            "EPOCH 82 ...\n",
            "Training loss = 0.081\n",
            "Validation loss = 0.076\n",
            "STARTING EPOCH 83 ...\n",
            "EPOCH 83 ...\n",
            "Training loss = 0.080\n",
            "Validation loss = 0.074\n",
            "STARTING EPOCH 84 ...\n",
            "EPOCH 84 ...\n",
            "Training loss = 0.077\n",
            "Validation loss = 0.077\n",
            "STARTING EPOCH 85 ...\n",
            "EPOCH 85 ...\n",
            "Training loss = 0.077\n",
            "Validation loss = 0.075\n",
            "STARTING EPOCH 86 ...\n",
            "EPOCH 86 ...\n",
            "Training loss = 0.077\n",
            "Validation loss = 0.071\n",
            "STARTING EPOCH 87 ...\n",
            "EPOCH 87 ...\n",
            "Training loss = 0.077\n",
            "Validation loss = 0.075\n",
            "STARTING EPOCH 88 ...\n",
            "EPOCH 88 ...\n",
            "Training loss = 0.077\n",
            "Validation loss = 0.090\n",
            "STARTING EPOCH 89 ...\n",
            "EPOCH 89 ...\n",
            "Training loss = 0.325\n",
            "Validation loss = 0.424\n",
            "STARTING EPOCH 90 ...\n",
            "EPOCH 90 ...\n",
            "Training loss = 0.324\n",
            "Validation loss = 0.184\n",
            "STARTING EPOCH 91 ...\n",
            "EPOCH 91 ...\n",
            "Training loss = 0.123\n",
            "Validation loss = 0.090\n",
            "STARTING EPOCH 92 ...\n",
            "EPOCH 92 ...\n",
            "Training loss = 0.087\n",
            "Validation loss = 0.072\n",
            "STARTING EPOCH 93 ...\n",
            "EPOCH 93 ...\n",
            "Training loss = 0.078\n",
            "Validation loss = 0.065\n",
            "STARTING EPOCH 94 ...\n",
            "EPOCH 94 ...\n",
            "Training loss = 0.073\n",
            "Validation loss = 0.061\n",
            "STARTING EPOCH 95 ...\n",
            "EPOCH 95 ...\n",
            "Training loss = 0.070\n",
            "Validation loss = 0.059\n",
            "STARTING EPOCH 96 ...\n",
            "EPOCH 96 ...\n",
            "Training loss = 0.069\n",
            "Validation loss = 0.058\n",
            "STARTING EPOCH 97 ...\n",
            "EPOCH 97 ...\n",
            "Training loss = 0.069\n",
            "Validation loss = 0.058\n",
            "STARTING EPOCH 98 ...\n",
            "EPOCH 98 ...\n",
            "Training loss = 0.067\n",
            "Validation loss = 0.061\n",
            "STARTING EPOCH 99 ...\n",
            "EPOCH 99 ...\n",
            "Training loss = 0.066\n",
            "Validation loss = 0.058\n",
            "STARTING EPOCH 100 ...\n",
            "EPOCH 100 ...\n",
            "Training loss = 0.065\n",
            "Validation loss = 0.059\n",
            "STARTING EPOCH 101 ...\n",
            "EPOCH 101 ...\n",
            "Training loss = 0.065\n",
            "Validation loss = 0.055\n",
            "STARTING EPOCH 102 ...\n",
            "EPOCH 102 ...\n",
            "Training loss = 0.065\n",
            "Validation loss = 0.053\n",
            "STARTING EPOCH 103 ...\n",
            "EPOCH 103 ...\n",
            "Training loss = 0.063\n",
            "Validation loss = 0.056\n",
            "STARTING EPOCH 104 ...\n",
            "EPOCH 104 ...\n",
            "Training loss = 0.061\n",
            "Validation loss = 0.055\n",
            "STARTING EPOCH 105 ...\n",
            "EPOCH 105 ...\n",
            "Training loss = 0.060\n",
            "Validation loss = 0.052\n",
            "STARTING EPOCH 106 ...\n",
            "EPOCH 106 ...\n",
            "Training loss = 0.061\n",
            "Validation loss = 0.053\n",
            "STARTING EPOCH 107 ...\n",
            "EPOCH 107 ...\n",
            "Training loss = 0.061\n",
            "Validation loss = 0.056\n",
            "STARTING EPOCH 108 ...\n",
            "EPOCH 108 ...\n",
            "Training loss = 0.060\n",
            "Validation loss = 0.056\n",
            "STARTING EPOCH 109 ...\n",
            "EPOCH 109 ...\n",
            "Training loss = 0.058\n",
            "Validation loss = 0.055\n",
            "STARTING EPOCH 110 ...\n",
            "EPOCH 110 ...\n",
            "Training loss = 0.058\n",
            "Validation loss = 0.060\n",
            "STARTING EPOCH 111 ...\n",
            "EPOCH 111 ...\n",
            "Training loss = 0.058\n",
            "Validation loss = 0.056\n",
            "STARTING EPOCH 112 ...\n",
            "EPOCH 112 ...\n",
            "Training loss = 0.058\n",
            "Validation loss = 0.052\n",
            "STARTING EPOCH 113 ...\n",
            "EPOCH 113 ...\n",
            "Training loss = 0.057\n",
            "Validation loss = 0.051\n",
            "STARTING EPOCH 114 ...\n",
            "EPOCH 114 ...\n",
            "Training loss = 0.056\n",
            "Validation loss = 0.049\n",
            "STARTING EPOCH 115 ...\n",
            "EPOCH 115 ...\n",
            "Training loss = 0.054\n",
            "Validation loss = 0.050\n",
            "STARTING EPOCH 116 ...\n",
            "EPOCH 116 ...\n",
            "Training loss = 0.054\n",
            "Validation loss = 0.053\n",
            "STARTING EPOCH 117 ...\n",
            "EPOCH 117 ...\n",
            "Training loss = 0.053\n",
            "Validation loss = 0.053\n",
            "STARTING EPOCH 118 ...\n",
            "EPOCH 118 ...\n",
            "Training loss = 0.054\n",
            "Validation loss = 0.049\n",
            "STARTING EPOCH 119 ...\n",
            "EPOCH 119 ...\n",
            "Training loss = 0.052\n",
            "Validation loss = 0.045\n",
            "STARTING EPOCH 120 ...\n",
            "EPOCH 120 ...\n",
            "Training loss = 0.052\n",
            "Validation loss = 0.047\n",
            "Checkpoint saved: /content/drive/My Drive/dataset/FCN/saved_model/20200213-075842/epoch120.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}