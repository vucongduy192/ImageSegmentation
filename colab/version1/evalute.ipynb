{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evalute.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9blIDYhiQaAP",
        "colab_type": "text"
      },
      "source": [
        "## **Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iadqlzfyQZO4",
        "colab_type": "code",
        "outputId": "212236e5-28e0-4005-9562-79ea0344deb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkulkHtJlRmH",
        "colab_type": "code",
        "outputId": "f2507ae0-6e8e-446c-a047-0dd24ecd2d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "from glob import glob\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "print('Tensorflow version: {}'.format(tf.__version__))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "538z8fxwQEgL",
        "colab_type": "text"
      },
      "source": [
        "## **FCN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uaoyfoTQHEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FCNVGG:\n",
        "    # ---------------------------------------------------------------------------\n",
        "    def __init__(self, session, num_classes):\n",
        "        self.session = session\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build_from_metagraph(self, metagraph_file, checkpoint_file):\n",
        "        self.__load_fcnvgg(metagraph_file, checkpoint_file)\n",
        "\n",
        "    def build_from_vgg(self, vgg_path):\n",
        "        self.__load_vgg(vgg_path)\n",
        "        self.__make_result_tensors()\n",
        "\n",
        "    def __load_fcnvgg(self, metagraph_file, checkpoint_file):\n",
        "        saver = tf.train.import_meta_graph(metagraph_file)\n",
        "        saver.restore(self.session, checkpoint_file)\n",
        "\n",
        "        self.image_input = self.session.graph.get_tensor_by_name('image_input:0')\n",
        "        self.keep_prob = self.session.graph.get_tensor_by_name('keep_prob:0')\n",
        "        self.logits      = self.session.graph.get_tensor_by_name('sum/fcn_logits:0')\n",
        "        self.softmax     = self.session.graph.get_tensor_by_name('result/fcn_softmax:0')\n",
        "        self.classes     = self.session.graph.get_tensor_by_name('result/fcn_class:0')\n",
        "\n",
        "\n",
        "    def __load_vgg(self, vgg_path):\n",
        "        model = tf.saved_model.loader.load(self.session, ['vgg16'], vgg_path)\n",
        "        graph = tf.get_default_graph()\n",
        "\n",
        "        self.image_input = graph.get_tensor_by_name('image_input:0')\n",
        "        self.keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
        "        self.vgg_layer3 = graph.get_tensor_by_name('layer3_out:0')\n",
        "        self.vgg_layer4 = graph.get_tensor_by_name('layer4_out:0')\n",
        "        self.vgg_layer7 = graph.get_tensor_by_name('layer7_out:0')\n",
        "\n",
        "    def __make_result_tensors(self):\n",
        "        \"\"\"\n",
        "        :param correct_label:\n",
        "        :param num_classes:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Use a shorter variable name for simplicity\n",
        "        layer3, layer4, layer7 = self.vgg_layer3, self.vgg_layer4, self.vgg_layer7\n",
        "        # Convert FCN to CONV\n",
        "        # Apply 1x1 convolution in place of fully connected layer\n",
        "        fcn8 = tf.layers.conv2d(layer7, filters=self.num_classes, kernel_size=1, name=\"fcn8\")\n",
        "        # Up-sample fcn8 with size depth=(4096?) to match size of layer 4\n",
        "        # so that we can add skip connection with 4th layer\n",
        "        fcn9 = tf.layers.conv2d_transpose(fcn8, filters=layer4.get_shape().as_list()[-1],\n",
        "                                          kernel_size=4, strides=(2, 2), padding='SAME', name=\"fcn9\")\n",
        "        # Add a skip connection between current final layer fcn8 and 4th layer\n",
        "        fcn9_skip_connected = tf.add(fcn9, layer4, name=\"fcn9_plus_vgg_layer4\")\n",
        "        # print(fcn9_skip_connected.shape)\n",
        "        # Up-sample again\n",
        "        fcn10 = tf.layers.conv2d_transpose(fcn9_skip_connected, filters=layer3.get_shape().as_list()[-1],\n",
        "                                           kernel_size=4, strides=(2, 2), padding='SAME', name=\"fcn10_conv2d\")\n",
        "\n",
        "        # Add skip connection\n",
        "        fcn10_skip_connected = tf.add(fcn10, layer3, name=\"fcn10_plus_vgg_layer3\")\n",
        "\n",
        "        # Up-sample again\n",
        "        # Final output: fcn11 = 8 * (4 * layer_out7 + 2 * layer_out4 + layer_out3)\n",
        "        fcn11 = tf.layers.conv2d_transpose(fcn10_skip_connected, filters=self.num_classes,\n",
        "                                           kernel_size=16, strides=(8, 8), padding='SAME', name=\"fcn11\")\n",
        "\n",
        "        with tf.variable_scope('sum'):\n",
        "            self.logits = tf.reshape(fcn11, (-1, self.num_classes), name=\"fcn_logits\")\n",
        "\n",
        "        with tf.name_scope('result'):\n",
        "            self.softmax = tf.nn.softmax(self.logits, name=\"fcn_softmax\")\n",
        "            self.classes = tf.argmax(self.softmax, axis=-1, name=\"fcn_class\")\n",
        "\n",
        "\n",
        "    def get_optimizer(self, labels, learning_rate=0.0001):\n",
        "        with tf.variable_scope('reshape'):\n",
        "            labels_reshaped  = tf.reshape(labels, [-1, self.num_classes])\n",
        "            logits_reshaped  = tf.reshape(self.logits, [-1, self.num_classes])\n",
        "\n",
        "            losses          = tf.nn.softmax_cross_entropy_with_logits(\n",
        "                                  labels=labels_reshaped,\n",
        "                                  logits=logits_reshaped)\n",
        "            loss            = tf.reduce_mean(losses)\n",
        "\n",
        "        with tf.variable_scope('optimizer'):\n",
        "            optimizer       = tf.train.AdamOptimizer(learning_rate)\n",
        "            optimizer       = optimizer.minimize(loss)\n",
        "\n",
        "        return optimizer, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QydgQ1nvQt-i",
        "colab_type": "text"
      },
      "source": [
        "## **VOC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXYqppg3pZIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def color_map(N=256, normalized=False):\n",
        "    def bitget(byteval, idx):\n",
        "        return (byteval & (1 << idx)) != 0\n",
        "\n",
        "    dtype = 'float32' if normalized else 'uint8'\n",
        "    cmap = np.zeros((N, 3), dtype=dtype)\n",
        "    for i in range(N):\n",
        "        r = g = b = 0\n",
        "        c = i\n",
        "        for j in range(8):\n",
        "            r = r | (bitget(c, 0) << 7 - j)\n",
        "            g = g | (bitget(c, 1) << 7 - j)\n",
        "            b = b | (bitget(c, 2) << 7 - j)\n",
        "            c = c >> 3\n",
        "\n",
        "        cmap[i] = np.array([b, g, r])\n",
        "\n",
        "    cmap = cmap / 255 if normalized else cmap\n",
        "    return cmap\n",
        "\n",
        "\n",
        "def color_map_dict():\n",
        "    labels = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair',\n",
        "              'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n",
        "              'train', 'tvmonitor', 'void']\n",
        "    \n",
        "    return dict(zip(labels[:-1], color_map()[:-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5HkaCRmpbBx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "d85e5827-7ffc-437c-ad95-e92d603e3110"
      },
      "source": [
        "print(color_map_dict())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'background': array([0, 0, 0], dtype=uint8), 'aeroplane': array([  0,   0, 128], dtype=uint8), 'bicycle': array([  0, 128,   0], dtype=uint8), 'bird': array([  0, 128, 128], dtype=uint8), 'boat': array([128,   0,   0], dtype=uint8), 'bottle': array([128,   0, 128], dtype=uint8), 'bus': array([128, 128,   0], dtype=uint8), 'car': array([128, 128, 128], dtype=uint8), 'cat': array([ 0,  0, 64], dtype=uint8), 'chair': array([  0,   0, 192], dtype=uint8), 'cow': array([  0, 128,  64], dtype=uint8), 'diningtable': array([  0, 128, 192], dtype=uint8), 'dog': array([128,   0,  64], dtype=uint8), 'horse': array([128,   0, 192], dtype=uint8), 'motorbike': array([128, 128,  64], dtype=uint8), 'person': array([128, 128, 192], dtype=uint8), 'pottedplant': array([ 0, 64,  0], dtype=uint8), 'sheep': array([  0,  64, 128], dtype=uint8), 'sofa': array([  0, 192,   0], dtype=uint8), 'train': array([  0, 192, 128], dtype=uint8), 'tvmonitor': array([128,  64,   0], dtype=uint8)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyqslqPUQvHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VOCSource(object):\n",
        "    def __init__(self):\n",
        "        self.num_training = None\n",
        "        self.num_validation = None\n",
        "        self.train_generator = None\n",
        "        self.valid_generator = None\n",
        "\n",
        "        self.num_testing = None\n",
        "        self.test_generator = None\n",
        "\n",
        "        self.label_colors = color_map_dict()\n",
        "        self.num_classes = len(self.label_colors)\n",
        "        self.image_size = (224, 224)\n",
        "\n",
        "    # -------------------------------------------------------------------------------\n",
        "    def load_data(self, data_dir, images_txt, validation_size=None):\n",
        "        images_dir = os.path.join(data_dir, 'VOC2012/JPEGImages/')   # very large\n",
        "        labels_dir = os.path.join(data_dir, 'VOC2012/SegmentationClass/')  # 2913\n",
        "        print(len(glob(images_dir)))\n",
        "        print(len(glob(labels_dir)))\n",
        "\n",
        "        # train.txt: 1464 , val.txt: 1449 , Segmentation: 2913\n",
        "        with open(images_txt, 'r') as f:\n",
        "            image_names = f.readlines()\n",
        "            image_paths, label_paths = [], {}\n",
        "            for image in image_names:\n",
        "                image_path = images_dir + image[:-1] + '.jpg'\n",
        "                image_paths.append(image_path)\n",
        "                label_paths[os.path.basename(image_path)] = labels_dir + image[:-1] + '.png'\n",
        "        \n",
        "        random.shuffle(image_paths)\n",
        "        if validation_size is None:\n",
        "            test_images = image_paths\n",
        "            self.num_testing = len(test_images)\n",
        "            self.test_generator = self.batch_generator(test_images, label_paths)\n",
        "        else:\n",
        "            num_images = len(image_paths)\n",
        "            valid_images = image_paths[:int(validation_size * num_images)]\n",
        "            train_images = image_paths[int(validation_size * num_images):]\n",
        "\n",
        "            self.num_training = len(train_images)\n",
        "            self.num_validation = len(valid_images)\n",
        "            self.train_generator = self.batch_generator(train_images, label_paths)\n",
        "            self.valid_generator = self.batch_generator(valid_images, label_paths)\n",
        "\n",
        "    # -------------------------------------------------------------------------------\n",
        "    def batch_generator(self, image_paths, label_paths):\n",
        "        def gen_batch(batch_size):\n",
        "            \"\"\"\n",
        "            :param batch_size:\n",
        "            :return: generator of a batch contain (images, labels)\n",
        "            \"\"\"\n",
        "            random.shuffle(image_paths)\n",
        "            for offset in range(0, len(image_paths), batch_size):\n",
        "                files = image_paths[offset:(offset + batch_size)]\n",
        "                images = []\n",
        "                labels = []\n",
        "                for image_file in files:\n",
        "                    label_file = label_paths[os.path.basename(image_file)]  # base name get file name from path\n",
        "                    image = cv2.resize(cv2.imread(image_file), self.image_size)\n",
        "                    label = cv2.resize(cv2.imread(label_file), self.image_size)\n",
        "\n",
        "                    label_bg   = np.zeros([image.shape[0], image.shape[1]], dtype=bool)\n",
        "                    label_list = []\n",
        "                    for obj, color_bgr in self.label_colors.items():\n",
        "                        if obj == 'background':\n",
        "                            continue\n",
        "                        label_current  = np.all(label == color_bgr, axis=2)\n",
        "                        label_bg      |= label_current\n",
        "                        label_list.append(label_current)\n",
        "\n",
        "                    label_bg   = ~label_bg\n",
        "                    label_all  = np.dstack([label_bg, *label_list])\n",
        "                    label_all  = label_all.astype(np.float32)\n",
        "\n",
        "                    # label = cv2.resize(cv2.imread(label_file), self.image_size)\n",
        "                    # label_obj = []\n",
        "                    # for obj, color_bgr in self.label_colors.items():\n",
        "                    #     obj = np.all(label == color_bgr, axis=2)\n",
        "                    #     label_obj.append(obj)\n",
        "\n",
        "                    # label_all = np.dstack(label_obj)\n",
        "                    # label_all = label_all.astype(np.float32)\n",
        "\n",
        "                    images.append(image.astype(np.float32))\n",
        "                    labels.append(label_all)\n",
        "                    \n",
        "                image = np.asarray(image, np.float32)\n",
        "                yield np.array(images), np.array(labels)\n",
        "\n",
        "        return gen_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qi6u7IQQzkb",
        "colab_type": "text"
      },
      "source": [
        "## **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uq9RAzZQzD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from source_kitti import KittiSource\n",
        "# from source_voc import VOCSource\n",
        "# from fcnvgg import FCNVGG\n",
        "\n",
        "\n",
        "def load_kitti_source():\n",
        "    return KittiSource()\n",
        "\n",
        "\n",
        "def load_voc_source():\n",
        "    return VOCSource()\n",
        "\n",
        "\n",
        "def load_fcnvgg(session, num_classes):\n",
        "    return FCNVGG(session, num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKJfDt2eQ4ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = '/content/drive/My Drive/dataset/FCN/VOCdevkit/'\n",
        "vgg_dir = '/content/drive/My Drive/dataset/FCN/vgg/'\n",
        "output_dir = '/content/drive/My Drive/dataset/FCN/output/'\n",
        "\n",
        "log_dir = '/content/drive/My Drive/dataset/FCN/graphs/'\n",
        "model_dir = '/content/drive/My Drive/dataset/FCN/saved_model/20200213-075842/'\n",
        "\n",
        "test_txt = '/content/drive/My Drive/dataset/FCN/VOCdevkit/VOC2012/ImageSets/Segmentation/val.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs5alcEJRJHn",
        "colab_type": "text"
      },
      "source": [
        "## **Evalute**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh-VzJ9NVmXl",
        "colab_type": "code",
        "outputId": "86b8813a-e0db-46ce-8e45-d8fc689f95b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch_size = 12\n",
        "source = load_voc_source()\n",
        "source.load_data(data_dir, test_txt)\n",
        "test_generator = source.test_generator"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arg9YOFNy40K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# img_label.shape, gt_label.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmSgkPTv0hPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_accuracy(eval_img, gt_img):\n",
        "    return np.array([\n",
        "      cal_pixel_accuracy(img_classed[idx], gt_batch[idx]),\n",
        "      cal_mean_pixel_accuracy(img_classed[idx], gt_batch[idx]),\n",
        "      cal_mean_IU(img_classed[idx], gt_batch[idx])\n",
        "    ])\n",
        "\n",
        "# Intersection-Over-Union accuracy\n",
        "def cal_pixel_accuracy(eval_img, gt_img):\n",
        "    \"\"\"\n",
        "        Calculate accuracy for all class one times:\n",
        "            - Extract all class appear in ground truth image\n",
        "            - sum_ii: pixel of class appear in both ground trush && evalute\n",
        "            - sum_i : pixel of class appear in ground truth\n",
        "\n",
        "        input: (224, 224) \n",
        "        return: pixel_accuracy\n",
        "    \"\"\"\n",
        "    classes, num_classes = extract_classes(gt_img)\n",
        "    eval_mask = extract_masks(eval_img, classes, num_classes)\n",
        "    gt_mask   = extract_masks(gt_img, classes, num_classes)\n",
        "    \n",
        "    sum_ii = 0\n",
        "    sum_i = 0\n",
        "    \n",
        "    # classes in ground truth image\n",
        "    for idx, cl in enumerate(classes):\n",
        "        curr_eval_mask = eval_mask[idx, :, :]\n",
        "        curr_gt_mask = gt_mask[idx, :, :]\n",
        "\n",
        "        sum_ii += np.sum(np.logical_and(curr_eval_mask, curr_gt_mask))  \n",
        "        sum_i  += np.sum(curr_gt_mask)\n",
        "    \n",
        "    if sum_i == 0:\n",
        "        pixel_accuracy = 0\n",
        "    else:        \n",
        "        pixel_accuracy = sum_ii / sum_i\n",
        "    \n",
        "    return pixel_accuracy\n",
        "\n",
        "def cal_mean_pixel_accuracy(eval_img, gt_img):\n",
        "    \"\"\"\n",
        "        Calculate accurcay for each class, then return average:\n",
        "            - Extract all class appear in ground truth image\n",
        "            - sum_ii: pixel of class appear in both ground trush && evalute\n",
        "            - sum_i : pixel of class appear in ground truth\n",
        "\n",
        "        input: (224, 224) \n",
        "        return mean_pixel_accuracy = sum_ii / sum_i\n",
        "    \"\"\"\n",
        "    classes, num_classes = extract_classes(gt_img)\n",
        "    eval_mask = extract_masks(eval_img, classes, num_classes)\n",
        "    gt_mask   = extract_masks(gt_img, classes, num_classes)\n",
        "\n",
        "    accuracy = list([0]) * num_classes\n",
        "\n",
        "    # classes in ground truth image\n",
        "    for idx, cl in enumerate(classes):\n",
        "        curr_eval_mask = eval_mask[idx, :, :]\n",
        "        curr_gt_mask = gt_mask[idx, :, :]\n",
        "\n",
        "        sum_ii = np.sum(np.logical_and(curr_eval_mask, curr_gt_mask))  \n",
        "        sum_i  = np.sum(curr_gt_mask)\n",
        "    \n",
        "        if sum_i != 0:\n",
        "            accuracy[idx] = sum_ii / sum_i\n",
        "\n",
        "    mean_accuracy = np.mean(accuracy)\n",
        "    return mean_accuracy\n",
        "\n",
        "def cal_mean_IU(eval_img, gt_img):\n",
        "    \"\"\"\n",
        "        Calculte with union class between eval_img & gt_img\n",
        "    \"\"\"\n",
        "    classes, num_classes = union_classes(eval_img, gt_img)\n",
        "    _, num_classes_gt = extract_classes(gt_img)\n",
        "\n",
        "    eval_mask = extract_masks(eval_img, classes, num_classes)\n",
        "    gt_mask   = extract_masks(gt_img, classes, num_classes)\n",
        "\n",
        "    IU = list([0]) * num_classes  # union classes\n",
        "\n",
        "    for idx, cl in enumerate(classes):\n",
        "        curr_eval_mask = eval_mask[idx, :, :]\n",
        "        curr_gt_mask = gt_mask[idx, :, :]\n",
        "\n",
        "        if (np.sum(curr_eval_mask) == 0) or (np.sum(curr_gt_mask) == 0):  # \n",
        "            continue\n",
        "        \n",
        "        sum_ii = np.sum(np.logical_and(curr_eval_mask, curr_gt_mask))\n",
        "        sum_i  = np.sum(curr_gt_mask)\n",
        "        sum_ij = np.sum(curr_eval_mask)\n",
        "\n",
        "        IU[idx] = sum_ii / (sum_i + sum_ij - sum_ii)\n",
        "\n",
        "    mean_IU = np.sum(IU) / num_classes_gt\n",
        "    return mean_IU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g9sdSbObuhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_classes(gt_img):\n",
        "    classes = np.unique(gt_img) # 0, 1, 2, .., 20\n",
        "    number_classes = len(classes)\n",
        "\n",
        "    return classes, number_classes\n",
        "\n",
        "def extract_masks(img, classes, number_classes):\n",
        "    height, width  = img.shape[0],  img.shape[1]\n",
        "    masks = np.zeros((number_classes, height, width)) # classes by ground truth image\n",
        "\n",
        "    for idx, cl in enumerate(classes):\n",
        "        masks[idx, :, :] = img == cl\n",
        "\n",
        "    return masks\n",
        "\n",
        "def union_classes(eval_img, gt_img):\n",
        "    eval_classes, _ = extract_classes(eval_img)\n",
        "    gt_classes, _   = extract_classes(gt_img)\n",
        "\n",
        "    classes = np.union1d(eval_classes, gt_classes)\n",
        "    num_classes = len(classes)\n",
        "\n",
        "    return classes, num_classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt3Ug1JHRDxi",
        "colab_type": "code",
        "outputId": "d0c11379-d4f4-4a56-ddc7-bb3f036b4d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "def draw_labels(img, label, label_colors):\n",
        "    img_save = np.zeros_like(img)\n",
        "    for val, color_bgr in label_colors.items(): # diningtable => rgb(124, 124, 0)\n",
        "        label_mask = label == list(label_colors.keys()).index(val)\n",
        "        img_save[label_mask] = color_bgr\n",
        "    return img_save\n",
        "\n",
        "state = tf.train.get_checkpoint_state(model_dir)\n",
        "if state is None:\n",
        "    print('[!] No network state found in ' + model_dir)\n",
        "    sys.exit(1)\n",
        "\n",
        "checkpoint_file = state.all_model_checkpoint_paths[-1]\n",
        "metagraph_file = checkpoint_file + '.meta'\n",
        "session_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "with tf.Session(config=session_config) as session:\n",
        "    session.run(tf.global_variables_initializer())\n",
        "\n",
        "    net = load_fcnvgg(session, source.num_classes)\n",
        "    net.build_from_metagraph(metagraph_file, checkpoint_file)\n",
        "    generator = test_generator(batch_size)\n",
        "\n",
        "    acc = np.array([0, 0 , 0], dtype=np.float64)\n",
        "    num_img = 0\n",
        "    \n",
        "    n_test_batches = int(math.ceil(source.num_testing/batch_size))\n",
        "    description = '[i] Processing evalute'\n",
        "\n",
        "    for X_batch, gt_batch in tqdm(generator, total=n_test_batches, desc=description, unit='batches'):\n",
        "        gt_batch = np.argmax(gt_batch, axis=3)\n",
        "        \n",
        "        feed = {net.image_input: X_batch, net.keep_prob: 1}\n",
        "        img_classed = session.run(net.classes, feed_dict=feed)\n",
        "        img_classed = img_classed.reshape(-1, source.image_size[1], \n",
        "                                          source.image_size[0])\n",
        "        batch_acc = np.array([0, 0, 0], dtype=np.float64)\n",
        "        for idx in range(X_batch.shape[0]):\n",
        "            img_label = draw_labels(X_batch[idx], img_classed[idx], source.label_colors)\n",
        "            gt_label = draw_labels(X_batch[idx], gt_batch[idx], source.label_colors)\n",
        "            # --------------------------------------- #\n",
        "            # Save sample output\n",
        "            # cv2.imwrite(output_dir + '{}.jpg'.format(idx+1), img_label)\n",
        "            # cv2.imwrite(output_dir + 'gt_{}.jpg'.format(idx+1), gt_label)\n",
        "            # cv2.imwrite(output_dir + 'source_{}.jpg'.format(idx+1), X_batch[idx])\n",
        "\n",
        "            # --------------------------------------- #\n",
        "            # Evalute model\n",
        "            batch_acc += cal_accuracy(img_classed[idx], gt_batch[idx])\n",
        "        acc += batch_acc\n",
        "        num_img += X_batch.shape[0]\n",
        "\n",
        "        if num_img % 120 == 0:\n",
        "            print('')\n",
        "            print('Pixel accuracy: {} ---- Mean pixel accuracy: {} ---- Mean IU: {}'.format(acc[0]/num_img, \n",
        "                                                                                            acc[1]/num_img, acc[2]/num_img))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/dataset/FCN/saved_model/20200213-075842/epoch120.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[i] Processing samples:   0%|          | 0/121 [00:00<?, ?batches/s]\u001b[A\n",
            "[i] Processing samples:   1%|          | 1/121 [00:10<21:02, 10.52s/batches]\u001b[A\n",
            "[i] Processing samples:   2%|▏         | 2/121 [00:20<20:43, 10.45s/batches]\u001b[A\n",
            "[i] Processing samples:   2%|▏         | 3/121 [00:30<20:23, 10.37s/batches]\u001b[A\n",
            "[i] Processing samples:   3%|▎         | 4/121 [00:41<20:10, 10.35s/batches]\u001b[A\n",
            "[i] Processing samples:   4%|▍         | 5/121 [00:51<19:56, 10.31s/batches]\u001b[A\n",
            "[i] Processing samples:   5%|▍         | 6/121 [01:01<19:42, 10.28s/batches]\u001b[A\n",
            "[i] Processing samples:   6%|▌         | 7/121 [01:11<19:25, 10.22s/batches]\u001b[A\n",
            "[i] Processing samples:   7%|▋         | 8/121 [01:22<19:17, 10.25s/batches]\u001b[A\n",
            "[i] Processing samples:   7%|▋         | 9/121 [01:32<19:05, 10.23s/batches]\u001b[A\n",
            "[i] Processing samples:   8%|▊         | 10/121 [01:42<18:57, 10.24s/batches]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pixel accuracy: 0.7712972005208335 ---- Mean pixel accuracy: 0.5282190702683793 ---- Mean IU: 0.45237972151678907\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[i] Processing samples:   9%|▉         | 11/121 [01:52<18:47, 10.25s/batches]\u001b[A\n",
            "[i] Processing samples:  10%|▉         | 12/121 [02:03<18:49, 10.36s/batches]\u001b[A\n",
            "[i] Processing samples:  11%|█         | 13/121 [02:13<18:36, 10.34s/batches]\u001b[A\n",
            "[i] Processing samples:  12%|█▏        | 14/121 [02:24<18:25, 10.34s/batches]\u001b[A\n",
            "[i] Processing samples:  12%|█▏        | 15/121 [02:34<18:09, 10.28s/batches]\u001b[A\n",
            "[i] Processing samples:  13%|█▎        | 16/121 [02:44<18:00, 10.29s/batches]\u001b[A\n",
            "[i] Processing samples:  14%|█▍        | 17/121 [02:55<17:55, 10.35s/batches]\u001b[A\n",
            "[i] Processing samples:  15%|█▍        | 18/121 [03:05<17:43, 10.32s/batches]\u001b[A\n",
            "[i] Processing samples:  16%|█▌        | 19/121 [03:15<17:26, 10.26s/batches]\u001b[A\n",
            "[i] Processing samples:  17%|█▋        | 20/121 [03:25<17:14, 10.24s/batches]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pixel accuracy: 0.7682616357089711 ---- Mean pixel accuracy: 0.5249098607863866 ---- Mean IU: 0.4499942528574321\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[i] Processing samples:  17%|█▋        | 21/121 [03:35<17:01, 10.22s/batches]\u001b[A\n",
            "[i] Processing samples:  18%|█▊        | 22/121 [03:45<16:50, 10.21s/batches]\u001b[A\n",
            "[i] Processing samples:  19%|█▉        | 23/121 [03:56<16:42, 10.23s/batches]\u001b[A\n",
            "[i] Processing samples:  20%|█▉        | 24/121 [04:06<16:30, 10.21s/batches]\u001b[A\n",
            "[i] Processing samples:  21%|██        | 25/121 [04:16<16:19, 10.20s/batches]\u001b[A\n",
            "[i] Processing samples:  21%|██▏       | 26/121 [04:26<16:09, 10.21s/batches]\u001b[A\n",
            "[i] Processing samples:  22%|██▏       | 27/121 [04:36<15:58, 10.19s/batches]\u001b[A\n",
            "[i] Processing samples:  23%|██▎       | 28/121 [04:47<15:47, 10.19s/batches]\u001b[A\n",
            "[i] Processing samples:  24%|██▍       | 29/121 [04:57<15:45, 10.28s/batches]\u001b[A\n",
            "[i] Processing samples:  25%|██▍       | 30/121 [05:07<15:32, 10.25s/batches]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pixel accuracy: 0.7738687597434806 ---- Mean pixel accuracy: 0.5149068025865621 ---- Mean IU: 0.44161237815564386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[i] Processing samples:  26%|██▌       | 31/121 [05:18<15:22, 10.25s/batches]\u001b[A\n",
            "[i] Processing samples:  26%|██▋       | 32/121 [05:28<15:09, 10.22s/batches]\u001b[A\n",
            "[i] Processing samples:  27%|██▋       | 33/121 [05:38<15:00, 10.23s/batches]\u001b[A\n",
            "[i] Processing samples:  28%|██▊       | 34/121 [05:48<14:49, 10.23s/batches]\u001b[A\n",
            "[i] Processing samples:  29%|██▉       | 35/121 [05:58<14:40, 10.24s/batches]\u001b[A\n",
            "[i] Processing samples:  30%|██▉       | 36/121 [06:09<14:26, 10.19s/batches]\u001b[A\n",
            "[i] Processing samples:  31%|███       | 37/121 [06:19<14:17, 10.21s/batches]\u001b[A\n",
            "[i] Processing samples:  31%|███▏      | 38/121 [06:29<14:11, 10.26s/batches]\u001b[A\n",
            "[i] Processing samples:  32%|███▏      | 39/121 [06:39<14:01, 10.26s/batches]\u001b[A\n",
            "[i] Processing samples:  33%|███▎      | 40/121 [06:50<13:48, 10.23s/batches]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pixel accuracy: 0.7784838618064412 ---- Mean pixel accuracy: 0.5159727750553815 ---- Mean IU: 0.443222030257808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[i] Processing samples:  34%|███▍      | 41/121 [07:00<13:39, 10.25s/batches]\u001b[A\n",
            "[i] Processing samples:  35%|███▍      | 42/121 [07:10<13:26, 10.21s/batches]\u001b[A\n",
            "[i] Processing samples:  36%|███▌      | 43/121 [07:20<13:17, 10.23s/batches]\u001b[A\n",
            "[i] Processing samples:  36%|███▋      | 44/121 [07:30<13:05, 10.20s/batches]\u001b[A\n",
            "[i] Processing samples:  37%|███▋      | 45/121 [07:41<12:57, 10.23s/batches]\u001b[A\n",
            "[i] Processing samples:  38%|███▊      | 46/121 [07:51<12:47, 10.23s/batches]\u001b[A\n",
            "[i] Processing samples:  39%|███▉      | 47/121 [08:01<12:39, 10.26s/batches]\u001b[A\n",
            "[i] Processing samples:  40%|███▉      | 48/121 [08:11<12:27, 10.24s/batches]\u001b[A\n",
            "[i] Processing samples:  40%|████      | 49/121 [08:22<12:17, 10.24s/batches]\u001b[A\n",
            "[i] Processing samples:  41%|████▏     | 50/121 [08:32<12:06, 10.23s/batches]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pixel accuracy: 0.7786188283907312 ---- Mean pixel accuracy: 0.5192308780568217 ---- Mean IU: 0.44544844956808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[i] Processing samples:  42%|████▏     | 51/121 [08:42<11:56, 10.23s/batches]\u001b[A\n",
            "[i] Processing samples:  43%|████▎     | 52/121 [08:52<11:46, 10.24s/batches]\u001b[A\n",
            "[i] Processing samples:  44%|████▍     | 53/121 [09:03<11:36, 10.24s/batches]\u001b[A\n",
            "[i] Processing samples:  45%|████▍     | 54/121 [09:13<11:24, 10.21s/batches]\u001b[A\n",
            "[i] Processing samples:  45%|████▌     | 55/121 [09:23<11:13, 10.21s/batches]\u001b[A\n",
            "[i] Processing samples:  46%|████▋     | 56/121 [09:33<11:02, 10.20s/batches]\u001b[A\n",
            "[i] Processing samples:  47%|████▋     | 57/121 [09:43<10:52, 10.19s/batches]\u001b[A\n",
            "[i] Processing samples:  48%|████▊     | 58/121 [09:54<10:42, 10.20s/batches]\u001b[A\n",
            "[i] Processing samples:  49%|████▉     | 59/121 [10:04<10:32, 10.20s/batches]\u001b[A\n",
            "[i] Processing samples:  50%|████▉     | 60/121 [10:14<10:20, 10.17s/batches]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pixel accuracy: 0.7818198829010772 ---- Mean pixel accuracy: 0.5169235436544141 ---- Mean IU: 0.4443136627934856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[i] Processing samples:  50%|█████     | 61/121 [10:24<10:11, 10.18s/batches]\u001b[A\n",
            "[i] Processing samples:  51%|█████     | 62/121 [10:34<09:58, 10.15s/batches]\u001b[A\n",
            "[i] Processing samples:  52%|█████▏    | 63/121 [10:44<09:50, 10.18s/batches]\u001b[A\n",
            "[i] Processing samples:  53%|█████▎    | 64/121 [10:55<09:40, 10.19s/batches]\u001b[A\n",
            "[i] Processing samples:  54%|█████▎    | 65/121 [11:05<09:30, 10.19s/batches]\u001b[A\n",
            "[i] Processing samples:  55%|█████▍    | 66/121 [11:15<09:18, 10.16s/batches]\u001b[A\n",
            "[i] Processing samples:  55%|█████▌    | 67/121 [11:25<09:08, 10.15s/batches]\u001b[A\n",
            "[i] Processing samples:  56%|█████▌    | 68/121 [11:35<08:57, 10.14s/batches]\u001b[A\n",
            "[i] Processing samples:  57%|█████▋    | 69/121 [11:45<08:48, 10.17s/batches]\u001b[A\n",
            "[i] Processing samples:  58%|█████▊    | 70/121 [11:56<08:39, 10.19s/batches]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pixel accuracy: 0.7795270647321427 ---- Mean pixel accuracy: 0.5113990456011163 ---- Mean IU: 0.4392787128495805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[i] Processing samples:  59%|█████▊    | 71/121 [12:06<08:28, 10.17s/batches]\u001b[A\n",
            "[i] Processing samples:  60%|█████▉    | 72/121 [12:16<08:17, 10.16s/batches]\u001b[A\n",
            "[i] Processing samples:  60%|██████    | 73/121 [12:26<08:07, 10.16s/batches]\u001b[A\n",
            "[i] Processing samples:  61%|██████    | 74/121 [12:36<07:57, 10.15s/batches]\u001b[A\n",
            "[i] Processing samples:  62%|██████▏   | 75/121 [12:46<07:47, 10.15s/batches]\u001b[A\n",
            "[i] Processing samples:  63%|██████▎   | 76/121 [12:57<07:38, 10.18s/batches]\u001b[A\n",
            "[i] Processing samples:  64%|██████▎   | 77/121 [13:07<07:27, 10.17s/batches]\u001b[A\n",
            "[i] Processing samples:  64%|██████▍   | 78/121 [13:17<07:18, 10.19s/batches]\u001b[A\n",
            "[i] Processing samples:  65%|██████▌   | 79/121 [13:27<07:08, 10.21s/batches]\u001b[A\n",
            "[i] Processing samples:  66%|██████▌   | 80/121 [13:37<06:58, 10.20s/batches]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pixel accuracy: 0.7807981945219493 ---- Mean pixel accuracy: 0.5110507634550782 ---- Mean IU: 0.43977286629135687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[i] Processing samples:  67%|██████▋   | 81/121 [13:48<06:47, 10.20s/batches]\u001b[A\n",
            "[i] Processing samples:  68%|██████▊   | 82/121 [13:58<06:38, 10.22s/batches]\u001b[A\n",
            "[i] Processing samples:  69%|██████▊   | 83/121 [14:08<06:26, 10.18s/batches]\u001b[A\n",
            "[i] Processing samples:  69%|██████▉   | 84/121 [14:18<06:16, 10.17s/batches]\u001b[A\n",
            "[i] Processing samples:  70%|███████   | 85/121 [14:28<06:05, 10.15s/batches]\u001b[A\n",
            "[i] Processing samples:  71%|███████   | 86/121 [14:38<05:55, 10.16s/batches]\u001b[A\n",
            "[i] Processing samples:  72%|███████▏  | 87/121 [14:49<05:46, 10.18s/batches]\u001b[A\n",
            "[i] Processing samples:  73%|███████▎  | 88/121 [14:59<05:36, 10.19s/batches]\u001b[A\n",
            "[i] Processing samples:  74%|███████▎  | 89/121 [15:09<05:24, 10.15s/batches]\u001b[A\n",
            "[i] Processing samples:  74%|███████▍  | 90/121 [15:19<05:14, 10.15s/batches]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pixel accuracy: 0.782152342273715 ---- Mean pixel accuracy: 0.5132631234618878 ---- Mean IU: 0.44175033423951887\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[i] Processing samples:  75%|███████▌  | 91/121 [15:29<05:04, 10.14s/batches]\u001b[A\n",
            "[i] Processing samples:  76%|███████▌  | 92/121 [15:41<05:11, 10.76s/batches]\u001b[A\n",
            "[i] Processing samples:  77%|███████▋  | 93/121 [15:52<04:56, 10.60s/batches]\u001b[A\n",
            "[i] Processing samples:  78%|███████▊  | 94/121 [16:02<04:42, 10.47s/batches]\u001b[A\n",
            "[i] Processing samples:  79%|███████▊  | 95/121 [16:12<04:28, 10.33s/batches]\u001b[A\n",
            "[i] Processing samples:  79%|███████▉  | 96/121 [16:22<04:17, 10.28s/batches]\u001b[A\n",
            "[i] Processing samples:  80%|████████  | 97/121 [16:32<04:05, 10.23s/batches]\u001b[A\n",
            "[i] Processing samples:  81%|████████  | 98/121 [16:42<03:54, 10.20s/batches]\u001b[A\n",
            "[i] Processing samples:  82%|████████▏ | 99/121 [16:52<03:43, 10.18s/batches]\u001b[A\n",
            "[i] Processing samples:  83%|████████▎ | 100/121 [17:02<03:33, 10.16s/batches]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pixel accuracy: 0.7830542822597786 ---- Mean pixel accuracy: 0.5142998240542138 ---- Mean IU: 0.4428607856465272\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[i] Processing samples:  83%|████████▎ | 101/121 [17:12<03:22, 10.13s/batches]\u001b[A\n",
            "[i] Processing samples:  84%|████████▍ | 102/121 [17:23<03:12, 10.13s/batches]\u001b[A\n",
            "[i] Processing samples:  85%|████████▌ | 103/121 [17:33<03:02, 10.12s/batches]\u001b[A\n",
            "[i] Processing samples:  86%|████████▌ | 104/121 [17:43<02:52, 10.12s/batches]\u001b[A\n",
            "[i] Processing samples:  87%|████████▋ | 105/121 [17:53<02:42, 10.13s/batches]\u001b[A\n",
            "[i] Processing samples:  88%|████████▊ | 106/121 [18:03<02:31, 10.12s/batches]\u001b[A\n",
            "[i] Processing samples:  88%|████████▊ | 107/121 [18:13<02:21, 10.10s/batches]\u001b[A\n",
            "[i] Processing samples:  89%|████████▉ | 108/121 [18:23<02:11, 10.12s/batches]\u001b[A\n",
            "[i] Processing samples:  90%|█████████ | 109/121 [18:33<02:01, 10.15s/batches]\u001b[A\n",
            "[i] Processing samples:  91%|█████████ | 110/121 [18:44<01:51, 10.14s/batches]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pixel accuracy: 0.7794932594841911 ---- Mean pixel accuracy: 0.514467507862231 ---- Mean IU: 0.4413584805669059\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[i] Processing samples:  92%|█████████▏| 111/121 [18:54<01:41, 10.15s/batches]\u001b[A\n",
            "[i] Processing samples:  93%|█████████▎| 112/121 [19:04<01:31, 10.15s/batches]\u001b[A\n",
            "[i] Processing samples:  93%|█████████▎| 113/121 [19:14<01:20, 10.10s/batches]\u001b[A\n",
            "[i] Processing samples:  94%|█████████▍| 114/121 [19:24<01:10, 10.10s/batches]\u001b[A\n",
            "[i] Processing samples:  95%|█████████▌| 115/121 [19:34<01:00, 10.09s/batches]\u001b[A\n",
            "[i] Processing samples:  96%|█████████▌| 116/121 [19:44<00:50, 10.09s/batches]\u001b[A\n",
            "[i] Processing samples:  97%|█████████▋| 117/121 [19:54<00:40, 10.10s/batches]\u001b[A\n",
            "[i] Processing samples:  98%|█████████▊| 118/121 [20:04<00:30, 10.12s/batches]\u001b[A\n",
            "[i] Processing samples:  98%|█████████▊| 119/121 [20:15<00:20, 10.11s/batches]\u001b[A\n",
            "[i] Processing samples:  99%|█████████▉| 120/121 [20:25<00:10, 10.11s/batches]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pixel accuracy: 0.7798293063970376 ---- Mean pixel accuracy: 0.5152852078182255 ---- Mean IU: 0.4421485734843674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[i] Processing samples: 100%|██████████| 121/121 [20:32<00:00,  9.37s/batches]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2FZ8pPYhxSx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c7d2f2f8-1e81-41aa-b76d-3d6c22d0fcdd"
      },
      "source": [
        "print (acc / source.num_testing)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.77951162 0.51481988 0.44164523]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrqkTeT2rWmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}